{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocessing Cornell Movie data.\n",
    "\n",
    "#### For Example.\n",
    "     Hello -> [padd,padd,padd,padd,hello]\n",
    "\n",
    "     Nice to meet you. -> [.,you,meet,to,nice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 Read Dataset\n",
      "Step2 Torkenize data\n",
      "Step3 Make dictionary\n",
      "Step4 Filter by length and padding\n",
      "Step5 bad word filtering and make X,y\n"
     ]
    }
   ],
   "source": [
    "#all preprocessing\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "''' \n",
    "    1. Read from 'movie-lines.txt'\n",
    "    2. Create a dictionary with ( key = line_id, value = text )\n",
    "'''\n",
    "def get_id2line():\n",
    "    lines=open('cornell_corpus/movie_lines.txt',encoding='iso-8859-1').read().split('\\n')\n",
    "    id2line = {}\n",
    "    for line in lines:\n",
    "        _line = line.split(' +++$+++ ')\n",
    "        if len(_line) == 5:\n",
    "            id2line[_line[0]] = _line[4]\n",
    "    return id2line\n",
    "\n",
    "'''\n",
    "    1. Read from 'movie_conversations.txt'\n",
    "    2. Create a list of [list of line_id's]\n",
    "'''\n",
    "def get_conversations():\n",
    "    conv_lines = open('cornell_corpus/movie_conversations.txt',encoding='iso-8859-1').read().split('\\n')\n",
    "    convs = [ ]\n",
    "    for line in conv_lines[:-1]:\n",
    "        _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "        convs.append(_line.split(','))\n",
    "    return convs\n",
    "\n",
    "'''\n",
    "    1. Get each conversation\n",
    "    2. Get each line from conversation\n",
    "    3. Save each conversation to file\n",
    "'''\n",
    "def extract_conversations(convs,id2line,path=''):\n",
    "    idx = 0\n",
    "    for conv in convs:\n",
    "        f_conv = open(path + str(idx)+'.txt', 'w')\n",
    "        for line_id in conv:\n",
    "            f_conv.write(id2line[line_id])\n",
    "            f_conv.write('\\n')\n",
    "        f_conv.close()\n",
    "        idx += 1\n",
    "\n",
    "'''\n",
    "    Get lists of all conversations as Questions and Answers\n",
    "    1. [questions]\n",
    "    2. [answers]\n",
    "'''\n",
    "def gather_dataset(convs, id2line):\n",
    "    questions = []; answers = []\n",
    "\n",
    "    for conv in convs:\n",
    "        if len(conv) %2 != 0:\n",
    "            conv = conv[:-1]\n",
    "        for i in range(len(conv)):\n",
    "            if i%2 == 0:\n",
    "                questions.append(id2line[conv[i]])\n",
    "            else:\n",
    "                answers.append(id2line[conv[i]])\n",
    "\n",
    "    return questions, answers\n",
    "\n",
    "\n",
    "'''\n",
    "    We need 4 files\n",
    "    1. train.enc : Encoder input for training\n",
    "    2. train.dec : Decoder input for training\n",
    "    3. test.enc  : Encoder input for testing\n",
    "    4. test.dec  : Decoder input for testing\n",
    "'''\n",
    "def prepare_seq2seq_files(questions, answers, path='',TESTSET_SIZE = 30000):\n",
    "    \n",
    "    # open files\n",
    "    train_enc = open(path + 'train.enc','w')\n",
    "    train_dec = open(path + 'train.dec','w')\n",
    "    test_enc  = open(path + 'test.enc', 'w')\n",
    "    test_dec  = open(path + 'test.dec', 'w')\n",
    "\n",
    "    # choose 30,000 (TESTSET_SIZE) items to put into testset\n",
    "    test_ids = random.sample([i for i in range(len(questions))],TESTSET_SIZE)\n",
    "\n",
    "    for i in range(len(questions)):\n",
    "        if i in test_ids:\n",
    "            test_enc.write(questions[i]+'\\n')\n",
    "            test_dec.write(answers[i]+ '\\n' )\n",
    "        else:\n",
    "            train_enc.write(questions[i]+'\\n')\n",
    "            train_dec.write(answers[i]+ '\\n' )\n",
    "        if i%10000 == 0:\n",
    "            print('\\n>> written %d lines' %(i))\n",
    "\n",
    "    # close files\n",
    "    train_enc.close()\n",
    "    train_dec.close()\n",
    "    test_enc.close()\n",
    "    test_dec.close()\n",
    "            \n",
    "\n",
    "def make_word_list(seq,q=True):\n",
    "    if q:word_list = [nltk.word_tokenize(s.replace(\"-\",\"\").replace(\"<\",\"\").replace(\">\",\"\").replace(\"<\",\"\").replace(\"[\",\"\").replace(\"]\",\"\"))[::-1] for s in seq]\n",
    "    else:word_list = [nltk.word_tokenize(s.replace(\"-\",\"\").replace(\"<\",\"\").replace(\">\",\"\").replace(\"<\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")) for s in seq]\n",
    "    return word_list\n",
    "\n",
    "def make_bad_filter(path='bad_words_list.txt'):\n",
    "    bad_word_filter = open(path,encoding='iso-8859-1').read().split('\\n')\n",
    "    return bad_word_filter\n",
    "\n",
    "def make_word2id(word_dic,bad_word_filter,word_list):\n",
    "    word_dic = word_dic.copy()\n",
    "    i = len(word_dic) ####\n",
    "    for w_list in word_list:\n",
    "        for word in w_list:\n",
    "            if word.lower() not in word_dic:\n",
    "                if word.lower() in bad_word_filter:\n",
    "                    word_dic[word.lower()] = -1\n",
    "                else:\n",
    "                    word_dic[word.lower()] = i\n",
    "                    i = i+1\n",
    "    return word_dic\n",
    "                 \n",
    "def make_word_len(word_list):\n",
    "    #長さのリストを作成\n",
    "    word_len = []\n",
    "    for i in range(len(word_list)):\n",
    "        word_len.append(len(word_list[i]))\n",
    "    return word_len\n",
    "\n",
    "def word_len_filter(q_list,a_list):\n",
    "    use_answer = []\n",
    "    use_question = []\n",
    "    ql_list_len = make_word_len(q_list)\n",
    "    al_list_len = make_word_len(a_list)\n",
    "    for i in range(len(a_list)):\n",
    "        if round(ql_list_len[i]/5)*5 <= 15 and al_list_len[i] < round(ql_list_len[i]/5)*5+5:\n",
    "            use_answer.append(a_list[i])\n",
    "            use_question.append(q_list[i])\n",
    "    use_question,use_answer =adjust_length(use_question,use_answer)\n",
    "    return use_question,use_answer\n",
    "     \n",
    "    \n",
    "def adjust_length(use_question,use_answer):\n",
    "    #PAD ~~~8~12\n",
    "    for i in range(len(use_question)):            \n",
    "        if len(use_question[i])<=5:j_max=5\n",
    "        elif len(use_question[i])<=10:j_max=10\n",
    "        elif len(use_question[i])<=15:j_max=15\n",
    "        elif len(use_question[i])<=20:j_max=20            \n",
    "        else: j_max = round(len(use_question[i])/5)*5 + 5            \n",
    "            \n",
    "        for j in range(j_max - len(use_question[i])):\n",
    "            use_question[i].insert(0,\"padd\")          \n",
    "        #STDGO~~~EOS PAD\n",
    "        \n",
    "    for i in range(len(use_answer)):\n",
    "        use_answer[i].insert(0,\"stdgo\")\n",
    "        use_answer[i].append(\"eos\")\n",
    "        if len(use_question[i])<=5:j_max=5\n",
    "        elif len(use_answer[i])<=10:j_max=10\n",
    "        elif len(use_answer[i])<=15:j_max=15\n",
    "        elif len(use_answer[i])<=20:j_max=20            \n",
    "        else: j_max = round(len(use_answer[i])/5)*5 + 5   \n",
    "            \n",
    "        for j in range(j_max  - len(use_answer[i])):\n",
    "            use_answer[i].append(\"padd\")          \n",
    "    return use_question,use_answer\n",
    "\n",
    "def filtering(bad_word_filter,word_dic,use_question,use_answer):\n",
    "    #Embed\n",
    "    X = [[word_dic[qes_word.lower()] for qes_word in question_inv] for question_inv in use_question]\n",
    "    y = [[word_dic[ans_word.lower()] for ans_word in answer] for answer in use_answer]\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    for i in range(len(X)):\n",
    "        if -1 not in X[i] and -1 not in y[i]:\n",
    "            X_list.append(X[i])\n",
    "            y_list.append(y[i])\n",
    "    X_lists = [[word_inv[qes_word] for qes_word in question_inv] for question_inv in X_list]\n",
    "    y_lists = [[word_inv[ans_word] for ans_word in answer] for answer in y_list]\n",
    "    X_list = np.asarray(X_lists)\n",
    "    y_list = np.asarray(y_lists)    \n",
    "    return X_list,y_list\n",
    "\n",
    "id2line = get_id2line()\n",
    "convs = get_conversations()\n",
    "print(\"Step1 Read Dataset\")\n",
    "questions, answers = gather_dataset(convs,id2line)\n",
    "\n",
    "print(\"Step2 Torkenize data\")\n",
    "question_list = make_word_list(questions,True)\n",
    "answer_list = make_word_list(questions,False)\n",
    "\n",
    "print(\"Step3 Make dictionary\")\n",
    "bad_word_filter = make_bad_filter()\n",
    "word_dic = make_word2id(word_dic={},\n",
    "                          bad_word_filter=bad_word_filter,\n",
    "                          word_list=question_list)\n",
    "word_dic = make_word2id(word_dic=word_dic,\n",
    "                          bad_word_filter=bad_word_filter,\n",
    "                          word_list=answer_list)\n",
    "\n",
    "word_dic[\"eos\"] = len(word_dic)\n",
    "word_dic[\"padd\"] = len(word_dic)\n",
    "word_dic[\"stdgo\"] = len(word_dic)\n",
    "word_dic[\"unk\"] = len(word_dic)\n",
    "word_inv = {v:k for k, v in word_dic.items()}\n",
    "\n",
    "print(\"Step4 Filter by length and padding\") \n",
    "use_question,use_answer = word_len_filter(question_list,answer_list)\n",
    "\n",
    "print(\"Step5 bad word filtering and make X,y\")\n",
    "X,y = filtering(bad_word_filter=bad_word_filter,\n",
    "          word_dic=word_dic,\n",
    "          use_question=use_question,\n",
    "          use_answer=use_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ ['padd', 'padd', 'padd', 'padd', '.', 'please', '.', 'part', 'spitting', 'and', 'gagging', 'and', 'hacking', 'the', 'not'],\n",
       "       ['padd', 'padd', 'padd', '?', 'again', 'name', 'your', \"'s\", 'what', '.', 'cute', 'so', \"'s\", 'that', '.', 'out', 'me', 'asking', \"'re\", 'you'],\n",
       "       ['introduction', 'proper', 'a', 'have', \"n't\", 'did', 'we', 'fault', 'my', \"'s\", 'it', ',', 'no', ',', 'no'],\n",
       "       ['padd', 'padd', 'padd', '?', 'why'],\n",
       "       ['padd', 'padd', 'padd', 'padd', '...', 'boyfriend', 'a', 'kat', 'find', 'could', 'we', 'only', 'if', ',', 'gosh']], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#write pickle\n",
    "import pickle\n",
    "with open('question_list.pickle', mode='wb') as f:\n",
    "    pickle.dump(question_list, f)\n",
    "with open('answer_list.pickle', mode='wb') as f:\n",
    "    pickle.dump(answer_list, f)\n",
    "with open('word_dic.pickle', mode='wb') as f:\n",
    "    pickle.dump(word_dic, f)\n",
    "with open('X.pickle', mode='wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "with open('y.pickle', mode='wb') as f:\n",
    "    pickle.dump(y, f)\n",
    "with open('bad_word_filter.pickle', mode='wb') as f:\n",
    "    pickle.dump(bad_word_filter, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#read pickle\n",
    "import pickle\n",
    "with open('X.pickle', mode='rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('y.pickle', mode='rb') as f:\n",
    "    y = pickle.load(f)\n",
    "with open('question_list.pickle', mode='rb') as f:\n",
    "    question_list = pickle.load(f)\n",
    "with open('answer_list.pickle', mode='rb') as f:\n",
    "    answer_list = pickle.load(f)\n",
    "with open('word_dic.pickle', mode='rb') as f:\n",
    "    word_dic = pickle.load(f)\n",
    "with open('bad_word_filter.pickle', mode='rb') as f:\n",
    "    bad_word_filter = pickle.load(f)\n",
    "word_inv = {v:k for k, v in word_dic.items()}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import Variable, optimizers, serializers, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from progressbar import ProgressBar\n",
    "import time\n",
    "\n",
    "# Encoder-Decode with Attention\n",
    "class Translator(chainer.Chain):\n",
    "    def __init__(self, debug = False, embed_size = 64):\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.source_lines, self.source_word2id, _                   = X,word_dic,word_inv\n",
    "        self.target_lines, self.target_word2id, self.target_id2word = y,word_dic,word_inv\n",
    "        source_size = len(self.source_word2id)\n",
    "        target_size = len(self.target_word2id)\n",
    "        super(Translator, self).__init__(\n",
    "            embed_x = L.EmbedID(source_size, embed_size),\n",
    "            embed_y = L.EmbedID(target_size, embed_size),\n",
    "            H       = L.LSTM(embed_size, embed_size),\n",
    "            Wc1     = L.Linear(embed_size, embed_size),\n",
    "            Wc2     = L.Linear(embed_size, embed_size),\n",
    "            W       = L.Linear(embed_size, target_size),\n",
    "        )\n",
    "        self.optimizer = optimizers.Adam()\n",
    "        self.optimizer.setup(self)\n",
    "\n",
    "        if debug:\n",
    "            print(\"embed_size: {0}\".format(embed_size), end=\"\")\n",
    "            print(\", source_size: {0}\".format(source_size), end=\"\")\n",
    "            print(\", target_size: {0}\".format(target_size))\n",
    "\n",
    "    def learn(self, debug = False):\n",
    "        line_num = len(self.source_lines) - 1\n",
    "        p = ProgressBar(maxval=line_num)  # 最大値100\n",
    "        for i in range(line_num):\n",
    "            source_words = self.source_lines[i]\n",
    "            target_words = self.target_lines[i]\n",
    "\n",
    "            self.H.reset_state()\n",
    "            self.zerograds()        \n",
    "            loss = self.loss(source_words, target_words)\n",
    "            loss.backward()\n",
    "            loss.unchain_backward()\n",
    "            self.optimizer.update()\n",
    "\n",
    "            if debug:\n",
    "                p.update(i+1)\n",
    "                time.sleep(0.01)\n",
    "                start_time = time.time()\n",
    "                \n",
    "    def updates(self,debug,X_mini,y_mini):\n",
    "        self.H.reset_state()\n",
    "        self.zerograds()        \n",
    "        loss = self.loss(X_mini, y_mini)\n",
    "        loss.backward()\n",
    "        loss.unchain_backward()\n",
    "        self.optimizer.update()        \n",
    "        \n",
    "    def test(self, source_words):\n",
    "        bar_h_i_list = self.h_i_list(source_words, True)\n",
    "        x_i = self.embed_x(Variable(np.array([self.source_word2id['unk']], dtype=np.int32), volatile='on'))\n",
    "        h_t = self.H(x_i)\n",
    "        c_t = self.c_t(bar_h_i_list, h_t.data[0], True)\n",
    "\n",
    "        result = []\n",
    "        bar_h_t = F.tanh(self.Wc1(c_t) + self.Wc2(h_t))\n",
    "        wid = np.argmax(F.softmax(self.W(bar_h_t)).data[0])\n",
    "        result.append(self.target_id2word[wid])\n",
    "\n",
    "        loop = 0\n",
    "        while (wid != self.target_word2id['unk']) and (loop <= 30):\n",
    "            y_i = self.embed_y(Variable(np.array([wid], dtype=np.int32), volatile='on'))\n",
    "            h_t = self.H(y_i)\n",
    "            c_t = self.c_t(bar_h_i_list, h_t.data, True)\n",
    "\n",
    "            bar_h_t = F.tanh(self.Wc1(c_t) + self.Wc2(h_t))\n",
    "            wid = np.argmax(F.softmax(self.W(bar_h_t)).data[0])\n",
    "            result.append(self.target_id2word[wid])\n",
    "            loop += 1\n",
    "        return result\n",
    "\n",
    "    # loss \n",
    "    def loss(self, source_words, target_words):\n",
    "        bar_h_i_list = self.h_i_list(source_words)\n",
    "        x_i = self.embed_x(Variable(np.array([self.source_word2id['unk']], dtype=np.int32)))\n",
    "        h_t = self.H(x_i)\n",
    "        c_t = self.c_t(bar_h_i_list, h_t.data[0])\n",
    "\n",
    "        bar_h_t    = F.tanh(self.Wc1(c_t) + self.Wc2(h_t))\n",
    "        tx         = Variable(np.array([self.target_word2id[target_words[0]]], dtype=np.int32))\n",
    "        accum_loss = F.softmax_cross_entropy(self.W(bar_h_t), tx)\n",
    "        for i in range(len(target_words)):\n",
    "            wid = self.target_word2id[target_words[i]]\n",
    "            y_i = self.embed_y(Variable(np.array([wid], dtype=np.int32)))\n",
    "            h_t = self.H(y_i)\n",
    "            c_t = self.c_t(bar_h_i_list, h_t.data)\n",
    "\n",
    "            bar_h_t    = F.tanh(self.Wc1(c_t) + self.Wc2(h_t))\n",
    "            next_wid   = self.target_word2id['unk'] if (i == len(target_words) - 1) else self.target_word2id[target_words[i+1]]\n",
    "            tx         = Variable(np.array([next_wid], dtype=np.int32))\n",
    "            loss       = F.softmax_cross_entropy(self.W(bar_h_t), tx)\n",
    "            accum_loss = loss if accum_loss is None else accum_loss + loss\n",
    "        return accum_loss\n",
    "\n",
    "    # h_i \n",
    "    def h_i_list(self, words, test = False):\n",
    "        h_i_list = []\n",
    "        volatile = 'on' if test else 'off'\n",
    "        for word in words:\n",
    "            wid = self.source_word2id[word]\n",
    "            x_i = self.embed_x(Variable(np.array([wid], dtype=np.int32), volatile=volatile))\n",
    "            h_i = self.H(x_i)\n",
    "            h_i_list.append(np.copy(h_i.data[0]))\n",
    "        return h_i_list\n",
    "\n",
    "    # context vector c_t \n",
    "    def c_t(self, bar_h_i_list, h_t, test = False):\n",
    "        s = 0.0\n",
    "        for bar_h_i in bar_h_i_list:\n",
    "            s += np.exp(h_t.dot(bar_h_i))\n",
    "\n",
    "        c_t = np.zeros(self.embed_size)\n",
    "        for bar_h_i in bar_h_i_list:\n",
    "            alpha_t_i = np.exp(h_t.dot(bar_h_i)) / s\n",
    "            c_t += alpha_t_i * bar_h_i\n",
    "        volatile = 'on' if test else 'off'\n",
    "        c_t = Variable(np.array([c_t]).astype(np.float32), volatile=volatile)\n",
    "        return c_t\n",
    "\n",
    "    # load data\n",
    "    def load_language(self, filename,word_dic=None):\n",
    "        if word_dic == word_dic:\n",
    "            word2id = word_dic\n",
    "        else:\n",
    "            word2id = {}\n",
    "            \n",
    "        lines = open(filename).read().split('\\n')\n",
    "        for i in range(len(lines)):\n",
    "            sentence = lines[i].split()\n",
    "            for word in sentence:\n",
    "                if word not in word2id:\n",
    "                    word2id[word] = len(word2id)\n",
    "        word2id['unk'] = len(word2id)\n",
    "        id2word = {v:k for k, v in word2id.items()}\n",
    "        return [lines, word2id, id2word]\n",
    "\n",
    "    # load model\n",
    "    def load_model(self, filename):\n",
    "        serializers.load_npz(filename, self)\n",
    "\n",
    "    # write model\n",
    "    def save_model(self, filename):\n",
    "        serializers.save_npz(filename, self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model new start.\n",
      "embed_size: 64"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (   654 of 101939) |                 | Elapsed Time: 0:06:51 ETA: 16:58:25"
     ]
    }
   ],
   "source": [
    "#Usage need progressbar2\n",
    "\n",
    "\n",
    "import time\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"model new start.\")\n",
    "\n",
    "model = Translator(True)\n",
    "#model.load_model(\"learned/seq2seq_25.model\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"model new finished. elapsed_time: {0:.1f}[sec]\".format(elapsed_time))\n",
    "\n",
    "epoch_num = 100\n",
    "for epoch in range(epoch_num):\n",
    "    print(\"{0} / {1} Epoch start.\".format(epoch + 1, epoch_num))\n",
    "\n",
    "    # 学習を実施\n",
    "    model.learn(True)\n",
    "    modelfile = \"learned/seq2seq_\" + str(epoch+1) + \".model\"\n",
    "    model.save_model(modelfile)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    remaining_time = elapsed_time * (epoch_num - epoch - 1)\n",
    "    print(\"{0} / {1} Epoch finished.\".format(epoch + 1, epoch_num), end=\"\")\n",
    "    print(\" elapsed_time: {0:.1f}[sec]\".format(elapsed_time), end=\"\")\n",
    "    print(\" remaining_time: {0:.1f}[sec]\".format(remaining_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def input2X(seq,word_dic):\n",
    "    seq = nltk.word_tokenize(seq.lower())[::-1]\n",
    "    for s in seq:\n",
    "        if s not in seq:\n",
    "            word_dic[s] = word_dic[\"unk\"]\n",
    "    return seq\n",
    "\n",
    "def translation(model_path,inputs,word_dic,bad_word_filter):\n",
    "    X_test = input2X(inputs,word_dic=word_dic)\n",
    "    X_test = adjust_length_X(X_test)\n",
    "    model = Translator()\n",
    "    model.load_model(model_path)\n",
    "    text_list = []\n",
    "    for i in range(len(X_test)-1):\n",
    "        transfred = model.test(X_test[i])\n",
    "        text = \"\"\n",
    "        for word in transfred:\n",
    "            if word not in [\"stdgo\",\"eos\",\"padd\",\"unk\"]:\n",
    "                text = text + \" \" + word\n",
    "        text_list.append(text)\n",
    "        #print(\"No.\",i,\" \",text[1].upper()+text[2:])\n",
    "    text = bfilter(bad_word_filter=bad_word_filter, text_list=text_list)\n",
    "    #print(text)\n",
    "    return text_list,text\n",
    "\n",
    "def adjust_length_X(Xs):\n",
    "    #PAD ~~~8~12\n",
    "\n",
    "    if len(Xs)<=5:j_max=5\n",
    "    elif len(Xs)<=10:j_max=10\n",
    "    elif len(Xs)<=15:j_max=15\n",
    "    elif len(Xs)<=20:j_max=20\n",
    "    else: j_max = round(len(Xs)/5)*5 + 5\n",
    "\n",
    "    for j in range(j_max - len(Xs)):\n",
    "        Xs.insert(0,\"padd\")\n",
    "    return Xs\n",
    "\n",
    "def bfilter(bad_word_filter,text_list):\n",
    "    text = \"\"\n",
    "    j = 1\n",
    "    for i in range(len(text_list)):\n",
    "        if text == \"\":\n",
    "            for word in nltk.word_tokenize(text_list[-(i+1)]):\n",
    "                if word in bad_word_filter:\n",
    "                    j = 0\n",
    "            if j == 1:       \n",
    "                text = text_list[-(i+1)]\n",
    "    if text == \"\":\n",
    "        text = \"What's are you saying?\"\n",
    "    return text.replace('-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "\t\t\tAuto:   you watch the black of you\n",
      "Why?\n",
      "\t\t\tAuto:   her\n",
      "Oh, did she?\n",
      "\t\t\tAuto:   hours good at the days .\n",
      "Thanks \n",
      "\t\t\tAuto:   even use see the 's 's ' on the year !\n",
      "What?\n",
      "\t\t\tAuto:   her\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#predict\n",
    "from_text = input()\n",
    "while from_text:\n",
    "    a,b = translation(\"seq2seq_30.model\",from_text,word_dic,bad_word_filter) #\n",
    "    print(\"\\t\\t\\tAuto: \",(b))\n",
    "    from_text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
